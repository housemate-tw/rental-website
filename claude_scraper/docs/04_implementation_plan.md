# å¯¦ä½œè¨ˆåŠƒï¼šé€æ­¥é–‹ç™¼æŒ‡å—

**æ–‡ä»¶ç‰ˆæœ¬**: v1.0
**å»ºç«‹æ—¥æœŸ**: 2025-10-20
**è² è²¬ AI**: Claude (Sonnet 4.5)

---

## 1. å¯¦ä½œç­–ç•¥

### 1.1 é–‹ç™¼åŸå‰‡

1. **ç”±ç°¡å…¥ç¹** (Simple to Complex)
   - å…ˆå»ºç«‹æœ€å°å¯è¡Œç‰ˆæœ¬ (MVP)
   - é€æ­¥å¢åŠ åŠŸèƒ½

2. **æ¸¬è©¦é©…å‹•** (Test as You Go)
   - æ¯å€‹æ¨¡çµ„å®Œæˆå¾Œç«‹å³æ¸¬è©¦
   - ä¸ç­‰å…¨éƒ¨å®Œæˆæ‰æ¸¬è©¦

3. **æ–‡æª”åŒæ­¥** (Document as You Code)
   - é‚Šå¯«ç¨‹å¼é‚Šæ›´æ–° `99_changelog.md`
   - é‡åˆ°å•é¡Œæ›´æ–° `06_troubleshooting.md`

4. **å¢é‡äº¤ä»˜** (Incremental Delivery)
   - æ¯å€‹éšæ®µéƒ½ç”¢å‡ºå¯é‹è¡Œçš„ç‰ˆæœ¬
   - ä½¿ç”¨è€…å¯ææ—©çœ‹åˆ°é€²å±•

---

## 2. å¯¦ä½œéšæ®µ

### Phase 0: ç’°å¢ƒæº–å‚™ âœ…

**ç›®æ¨™**: ç¢ºä¿é–‹ç™¼ç’°å¢ƒå°±ç·’

**æ­¥é©Ÿ**:

```bash
# 1. ç¢ºèª Python ç‰ˆæœ¬
python3 --version  # éœ€è¦ 3.9+

# 2. å»ºç«‹è™›æ“¬ç’°å¢ƒ (å»ºè­°)
cd /Users/sabrina/Documents/housemate-finder-app/claude_scraper
python3 -m venv venv
source venv/bin/activate  # macOS/Linux

# 3. å®‰è£ä¾è³´
pip install playwright python-dotenv tqdm

# 4. å®‰è£ Playwright ç€è¦½å™¨
playwright install chromium

# 5. é©—è­‰ Gemini è…³æœ¬å¯åŸ·è¡Œ
python3 /Users/sabrina/Documents/rental_project/save_rental_v8.py
```

**é æœŸæ™‚é–“**: 10 åˆ†é˜

---

### Phase 1: æ ¸å¿ƒæ¨¡çµ„é–‹ç™¼ ğŸ”¨

#### 1.1 Logger System (å„ªå…ˆåº¦: æœ€é«˜)

**ç‚ºä½•å…ˆåš**: å¾ŒçºŒæ‰€æœ‰æ¨¡çµ„éƒ½éœ€è¦æ—¥èªŒ

**æª”æ¡ˆ**: `src/logger.py`

**å¯¦ä½œå…§å®¹**:

```python
import logging
import os
from datetime import datetime

class ScraperLogger:
    def __init__(self, log_dir='logs/'):
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)

        # å»ºç«‹ logger
        self.logger = logging.getLogger('scraper')
        self.logger.setLevel(logging.DEBUG)

        # å®Œæ•´æ—¥èªŒ
        today = datetime.now().strftime('%Y%m%d')
        fh_all = logging.FileHandler(
            f'{log_dir}/scraper_{today}.log',
            encoding='utf-8'
        )
        fh_all.setLevel(logging.DEBUG)

        # éŒ¯èª¤æ—¥èªŒ
        fh_error = logging.FileHandler(
            f'{log_dir}/error_{today}.log',
            encoding='utf-8'
        )
        fh_error.setLevel(logging.ERROR)

        # æ ¼å¼
        formatter = logging.Formatter(
            '[%(asctime)s] [%(levelname)s] [%(module)s] %(message)s'
        )
        fh_all.setFormatter(formatter)
        fh_error.setFormatter(formatter)

        self.logger.addHandler(fh_all)
        self.logger.addHandler(fh_error)

        # Console handler (å¯é¸)
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        ch.setFormatter(formatter)
        self.logger.addHandler(ch)

    def debug(self, message):
        self.logger.debug(message)

    def info(self, message):
        self.logger.info(message)

    def warning(self, message):
        self.logger.warning(message)

    def error(self, message, exc_info=False):
        self.logger.error(message, exc_info=exc_info)

    def critical(self, message, exc_info=False):
        self.logger.critical(message, exc_info=exc_info)
```

**æ¸¬è©¦**:

```python
# æ¸¬è©¦è…³æœ¬
logger = ScraperLogger('test_logs/')
logger.info("Test message")
logger.error("Test error")

# æª¢æŸ¥ test_logs/ æ˜¯å¦ç”¢ç”Ÿæ—¥èªŒæª”
```

**å®Œæˆæ¨™æº–**:
- âœ… æ—¥èªŒæª”æ­£ç¢ºç”¢ç”Ÿ
- âœ… æ ¼å¼æ­£ç¢º
- âœ… éŒ¯èª¤æ—¥èªŒåˆ†é›¢

**é æœŸæ™‚é–“**: 30 åˆ†é˜

---

#### 1.2 State Manager

**æª”æ¡ˆ**: `src/state_manager.py`

**å¯¦ä½œå…§å®¹**:

```python
import json
import os
from datetime import datetime

class StateManager:
    def __init__(self, state_file='state/scraper_state.json'):
        self.state_file = state_file
        os.makedirs(os.path.dirname(state_file), exist_ok=True)
        self.state = self._load_or_create()

    def _load_or_create(self):
        """è¼‰å…¥æˆ–å»ºç«‹æ–°ç‹€æ…‹æª”"""
        if os.path.exists(self.state_file):
            with open(self.state_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {
                'version': '1.0',
                'sessions': [],
                'processed_post_ids': [],
                'metadata': {
                    'total_all_time': 0
                }
            }

    def save(self):
        """å„²å­˜ç‹€æ…‹åˆ°æª”æ¡ˆ"""
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(self.state, f, ensure_ascii=False, indent=2)

    def start_session(self):
        """é–‹å§‹æ–° session"""
        session_id = datetime.now().strftime('%Y%m%d_%H%M%S')
        session = {
            'session_id': session_id,
            'start_time': datetime.now().isoformat(),
            'end_time': None,
            'status': 'running',
            'total_processed': 0,
            'total_failed': 0
        }
        self.state['sessions'].append(session)
        self.save()
        return session_id

    def end_session(self, session_id, status='completed'):
        """çµæŸ session"""
        for session in self.state['sessions']:
            if session['session_id'] == session_id:
                session['end_time'] = datetime.now().isoformat()
                session['status'] = status
                break
        self.save()

    def is_processed(self, post_id):
        """æª¢æŸ¥æ˜¯å¦å·²è™•ç†"""
        return post_id in [p['id'] for p in self.state['processed_post_ids']]

    def mark_processed(self, post_id, session_id):
        """æ¨™è¨˜ç‚ºå·²è™•ç†"""
        if not self.is_processed(post_id):
            self.state['processed_post_ids'].append({
                'id': post_id,
                'processed_at': datetime.now().isoformat(),
                'session_id': session_id
            })
            self.state['metadata']['total_all_time'] += 1

            # æ›´æ–° session çµ±è¨ˆ
            for session in self.state['sessions']:
                if session['session_id'] == session_id:
                    session['total_processed'] += 1
                    break

            self.save()

    def increment_failed(self, session_id):
        """å¢åŠ å¤±æ•—è¨ˆæ•¸"""
        for session in self.state['sessions']:
            if session['session_id'] == session_id:
                session['total_failed'] += 1
                break
        self.save()

    def get_stats(self):
        """å–å¾—çµ±è¨ˆè³‡è¨Š"""
        return {
            'total_all_time': self.state['metadata']['total_all_time'],
            'total_sessions': len(self.state['sessions']),
            'latest_session': self.state['sessions'][-1] if self.state['sessions'] else None
        }
```

**æ¸¬è©¦**:

```python
# æ¸¬è©¦è…³æœ¬
sm = StateManager('test_state/test.json')
session_id = sm.start_session()
sm.mark_processed('test_post_1', session_id)
assert sm.is_processed('test_post_1') == True
sm.end_session(session_id)
print(sm.get_stats())
```

**å®Œæˆæ¨™æº–**:
- âœ… ç‹€æ…‹æ­£ç¢ºå„²å­˜å’Œè¼‰å…¥
- âœ… å»é‡é‚è¼¯æ­£ç¢º
- âœ… çµ±è¨ˆè³‡è¨Šæ­£ç¢º

**é æœŸæ™‚é–“**: 45 åˆ†é˜

---

#### 1.3 Configuration Loader

**æª”æ¡ˆ**: `src/config.py`

**å¯¦ä½œå…§å®¹**:

```python
import json
import os

class Config:
    def __init__(self, config_path='config/config.json'):
        if not os.path.exists(config_path):
            raise FileNotFoundError(
                f"Config file not found: {config_path}. "
                f"Please copy config.example.json to config.json"
            )

        with open(config_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

    def get(self, key_path, default=None):
        """
        å–å¾—é…ç½®å€¼
        key_path æ ¼å¼: 'facebook.group_url'
        """
        keys = key_path.split('.')
        value = self.data

        for key in keys:
            if key in value:
                value = value[key]
            else:
                return default

        return value

    # ä¾¿æ·å±¬æ€§
    @property
    def group_url(self):
        return self.get('facebook.group_url')

    @property
    def max_posts(self):
        return self.get('scraper.max_posts_per_run', 500)

    @property
    def save_script_path(self):
        return self.get('paths.save_script')

    # ... å…¶ä»–å¸¸ç”¨é…ç½®
```

**é æœŸæ™‚é–“**: 20 åˆ†é˜

---

### Phase 2: ç€è¦½å™¨æ§åˆ¶ ğŸŒ

#### 2.1 åŸºç¤ç€è¦½å™¨å•Ÿå‹•

**æª”æ¡ˆ**: `src/browser.py`

**å¯¦ä½œå…§å®¹**:

```python
from playwright.sync_api import sync_playwright
import time
import random

class BrowserController:
    def __init__(self, config, logger):
        self.config = config
        self.logger = logger
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None

    def launch(self, headless=False):
        """å•Ÿå‹•ç€è¦½å™¨"""
        self.logger.info("Launching browser...")

        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(
            headless=headless,
            args=['--disable-blink-features=AutomationControlled']
        )

        self.logger.info("Browser launched successfully")

    def create_context(self, cookies_path=None):
        """å»ºç«‹ç€è¦½å™¨ context"""
        if cookies_path and os.path.exists(cookies_path):
            self.logger.info(f"Loading cookies from {cookies_path}")
            self.context = self.browser.new_context(
                storage_state=cookies_path,
                user_agent=self.config.get('facebook.user_agent'),
                viewport={'width': 1280, 'height': 720}
            )
        else:
            self.logger.info("Creating new context (no cookies)")
            self.context = self.browser.new_context(
                user_agent=self.config.get('facebook.user_agent'),
                viewport={'width': 1280, 'height': 720}
            )

        self.page = self.context.new_page()

    def save_cookies(self, path):
        """å„²å­˜ cookies"""
        self.logger.info(f"Saving cookies to {path}")
        self.context.storage_state(path=path)

    def goto(self, url, wait_until='networkidle'):
        """å°èˆªåˆ°ç¶²å€"""
        self.logger.info(f"Navigating to {url}")
        self.page.goto(url, wait_until=wait_until)

    def scroll_to_bottom(self, delay=None):
        """æ»¾å‹•åˆ°åº•éƒ¨"""
        if delay is None:
            delay = random.uniform(
                self.config.get('scraper.scroll_delay')[0],
                self.config.get('scraper.scroll_delay')[1]
            )

        old_height = self.page.evaluate("document.body.scrollHeight")
        self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        time.sleep(delay)

        new_height = self.page.evaluate("document.body.scrollHeight")
        return new_height > old_height  # æ˜¯å¦æœ‰æ–°å…§å®¹

    def wait_for_selector(self, selector, timeout=30000):
        """ç­‰å¾…å…ƒç´ å‡ºç¾"""
        try:
            self.page.wait_for_selector(selector, timeout=timeout)
            return True
        except Exception as e:
            self.logger.error(f"Timeout waiting for {selector}: {e}")
            return False

    def close(self):
        """é—œé–‰ç€è¦½å™¨"""
        if self.context:
            self.context.close()
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
        self.logger.info("Browser closed")
```

**æ¸¬è©¦**:

```python
# æ‰‹å‹•æ¸¬è©¦
from src.browser import BrowserController
from src.config import Config
from src.logger import ScraperLogger

config = Config()
logger = ScraperLogger()
browser = BrowserController(config, logger)

browser.launch(headless=False)
browser.create_context()
browser.goto('https://www.facebook.com')

# æ‰‹å‹•ç™»å…¥...
input("Press Enter after login...")

browser.save_cookies('config/auth.json')
browser.close()
```

**å®Œæˆæ¨™æº–**:
- âœ… ç€è¦½å™¨æˆåŠŸå•Ÿå‹•
- âœ… å¯å°èˆªåˆ° Facebook
- âœ… å¯å„²å­˜å’Œè¼‰å…¥ Cookies

**é æœŸæ™‚é–“**: 1 å°æ™‚

---

#### 2.2 è²¼æ–‡æå–

**æª”æ¡ˆ**: `src/extractor.py`

**å¯¦ä½œå…§å®¹**:

```python
class PostExtractor:
    def __init__(self, logger):
        self.logger = logger

    def extract_posts(self, page):
        """æå–é é¢ä¸Šæ‰€æœ‰è²¼æ–‡"""
        posts = page.query_selector_all('[role="article"]')
        self.logger.info(f"Found {len(posts)} posts on page")
        return posts

    def extract_post_data(self, post_element):
        """å¾è²¼æ–‡å…ƒç´ æå–è³‡æ–™"""
        try:
            # æå–è²¼æ–‡ ID (å¾ data-* å±¬æ€§æˆ– URL)
            post_id = self._extract_post_id(post_element)

            # æå–æ–‡æœ¬å…§å®¹
            text = self._extract_text(post_element)

            # æå– URL
            url = self._extract_url(post_element)

            # æå–æ™‚é–“æˆ³ (é¸ç”¨)
            timestamp = self._extract_timestamp(post_element)

            return {
                'id': post_id,
                'text': text,
                'url': url,
                'timestamp': timestamp
            }

        except Exception as e:
            self.logger.error(f"Failed to extract post data: {e}", exc_info=True)
            return None

    def _extract_post_id(self, element):
        """æå–è²¼æ–‡ ID"""
        # æ–¹æ³• 1: å¾ URL æå–
        link = element.query_selector('a[href*="/posts/"]')
        if link:
            href = link.get_attribute('href')
            # è§£æ URL å–å¾— post ID
            import re
            match = re.search(r'/posts/(\d+)', href)
            if match:
                return match.group(1)

        # æ–¹æ³• 2: å¾ data å±¬æ€§
        # ...

        # Fallback: ä½¿ç”¨å…§å®¹ hash
        import hashlib
        text = self._extract_text(element)
        return hashlib.md5(text.encode()).hexdigest()[:16]

    def _extract_text(self, element):
        """æå–è²¼æ–‡æ–‡æœ¬"""
        # å˜—è©¦å¤šç¨®é¸æ“‡å™¨
        selectors = [
            '[data-ad-comet-preview="message"]',
            '[data-ad-preview="message"]',
            'div[dir="auto"]'
        ]

        for selector in selectors:
            text_elem = element.query_selector(selector)
            if text_elem:
                return text_elem.inner_text()

        return ""

    def _extract_url(self, element):
        """æå–è²¼æ–‡ URL"""
        link = element.query_selector('a[href*="/posts/"], a[href*="permalink"]')
        if link:
            href = link.get_attribute('href')
            # è£œå…¨å®Œæ•´ URL
            if href.startswith('/'):
                return f"https://www.facebook.com{href}"
            return href
        return ""

    def _extract_timestamp(self, element):
        """æå–æ™‚é–“æˆ³ (é¸ç”¨)"""
        # ... å¯¦ä½œ ...
        return None
```

**æ¸¬è©¦**:

```python
# åœ¨å¯¦éš› Facebook é é¢æ¸¬è©¦
browser.goto(config.group_url)
extractor = PostExtractor(logger)
posts = extractor.extract_posts(browser.page)

if posts:
    data = extractor.extract_post_data(posts[0])
    print(data)
```

**å®Œæˆæ¨™æº–**:
- âœ… èƒ½æå–è²¼æ–‡ ID
- âœ… èƒ½æå–è²¼æ–‡æ–‡æœ¬
- âœ… èƒ½æå–è²¼æ–‡ URL

**é æœŸæ™‚é–“**: 1.5 å°æ™‚

---

### Phase 3: ä¸»ç¨‹å¼æ•´åˆ ğŸ”—

**æª”æ¡ˆ**: `src/scraper.py`

**å¯¦ä½œå…§å®¹**:

```python
from src.config import Config
from src.logger import ScraperLogger
from src.state_manager import StateManager
from src.browser import BrowserController
from src.extractor import PostExtractor
import subprocess
import time

class FacebookScraper:
    def __init__(self, config_path='config/config.json'):
        self.config = Config(config_path)
        self.logger = ScraperLogger(self.config.get('paths.log_dir'))
        self.state = StateManager(self.config.get('paths.state_file'))
        self.browser = BrowserController(self.config, self.logger)
        self.extractor = PostExtractor(self.logger)
        self.session_id = None

    def run(self):
        """ä¸»åŸ·è¡Œæµç¨‹"""
        try:
            self.logger.info("=== Scraper Started ===")
            self.session_id = self.state.start_session()

            # 1. å•Ÿå‹•ç€è¦½å™¨
            self.browser.launch(headless=self.config.get('scraper.headless', False))
            self.browser.create_context(self.config.get('facebook.cookies_path'))

            # 2. å°èˆªåˆ°ç¤¾åœ˜
            self.browser.goto(self.config.group_url)

            # 3. æª¢æŸ¥æ˜¯å¦éœ€è¦ç™»å…¥
            if not self._is_logged_in():
                self.logger.warning("Not logged in. Please login manually.")
                input("Press Enter after login...")
                self.browser.save_cookies(self.config.get('facebook.cookies_path'))

            # 4. åŸ·è¡Œçˆ¬å–
            self._scrape_loop()

            # 5. çµæŸ
            self.state.end_session(self.session_id, 'completed')
            self.logger.info("=== Scraper Completed ===")

        except KeyboardInterrupt:
            self.logger.warning("Interrupted by user")
            self.state.end_session(self.session_id, 'interrupted')

        except Exception as e:
            self.logger.critical(f"Fatal error: {e}", exc_info=True)
            self.state.end_session(self.session_id, 'failed')

        finally:
            self.browser.close()
            self._print_summary()

    def _is_logged_in(self):
        """æª¢æŸ¥æ˜¯å¦å·²ç™»å…¥"""
        # ç°¡å–®æª¢æŸ¥: çœ‹æ˜¯å¦æœ‰ä½¿ç”¨è€…é¸å–®
        return self.browser.page.query_selector('[aria-label*="Account"]') is not None

    def _scrape_loop(self):
        """çˆ¬å–ä¸»è¿´åœˆ"""
        max_posts = self.config.max_posts
        processed_count = 0

        while processed_count < max_posts:
            # 1. æå–ç•¶å‰é é¢çš„è²¼æ–‡
            posts = self.extractor.extract_posts(self.browser.page)

            # 2. è™•ç†æ¯ä¸€å‰‡è²¼æ–‡
            for post_elem in posts:
                if processed_count >= max_posts:
                    break

                post_data = self.extractor.extract_post_data(post_elem)
                if not post_data:
                    continue

                # 3. å»é‡
                if self.state.is_processed(post_data['id']):
                    self.logger.debug(f"Skipping duplicate: {post_data['id']}")
                    continue

                # 4. å„²å­˜
                if self._save_post(post_data):
                    self.state.mark_processed(post_data['id'], self.session_id)
                    processed_count += 1
                    self.logger.info(f"Processed {processed_count}/{max_posts}: {post_data['id']}")
                else:
                    self.state.increment_failed(self.session_id)

            # 5. æ»¾å‹•è¼‰å…¥æ›´å¤š
            has_more = self.browser.scroll_to_bottom()
            if not has_more:
                self.logger.info("Reached end of feed")
                break

            # 6. ç­‰å¾…æ–°å…§å®¹è¼‰å…¥
            time.sleep(2)

    def _save_post(self, post_data):
        """å„²å­˜è²¼æ–‡ (å‘¼å« Gemini è…³æœ¬)"""
        try:
            # ç°¡åŒ–ç‰ˆ: ç›´æ¥å‚³éåŸå§‹æ–‡æœ¬
            # å¯¦éš›ä½¿ç”¨æ™‚éœ€æ ¹æ“š save_rental_v8.py çš„åƒæ•¸èª¿æ•´

            cmd = [
                'python3',
                self.config.save_script_path,
                # ... åƒæ•¸ (å¾…å¯¦ä½œ) ...
                post_data['text'],
                post_data['url']
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)

            if result.returncode == 0:
                self.logger.debug(f"Saved: {result.stdout.strip()}")
                return True
            else:
                self.logger.error(f"Save failed: {result.stderr}")
                return False

        except Exception as e:
            self.logger.error(f"Exception saving post: {e}", exc_info=True)
            return False

    def _print_summary(self):
        """å°å‡ºåŸ·è¡Œæ‘˜è¦"""
        stats = self.state.get_stats()
        print("\n=== Summary ===")
        print(f"Total processed (this session): {stats['latest_session']['total_processed']}")
        print(f"Total failed (this session): {stats['latest_session']['total_failed']}")
        print(f"Total all time: {stats['total_all_time']}")


if __name__ == '__main__':
    scraper = FacebookScraper()
    scraper.run()
```

**å®Œæˆæ¨™æº–**:
- âœ… èƒ½å®Œæ•´åŸ·è¡Œçˆ¬å–æµç¨‹
- âœ… å»é‡æ­£ç¢º
- âœ… æ—¥èªŒå’Œç‹€æ…‹æ­£ç¢ºè¨˜éŒ„

**é æœŸæ™‚é–“**: 2 å°æ™‚

---

### Phase 4: æ¸¬è©¦èˆ‡å„ªåŒ– ğŸ§ª

#### 4.1 æ•´åˆæ¸¬è©¦

**æ¸¬è©¦é …ç›®**:

1. **å°è¦æ¨¡æ¸¬è©¦** (10 å‰‡è²¼æ–‡)
   ```bash
   # ä¿®æ”¹ config.json: max_posts_per_run = 10
   python3 src/scraper.py
   ```

2. **ä¸­æ–·æ¢å¾©æ¸¬è©¦**
   ```bash
   # åŸ·è¡Œåˆ°ä¸€åŠæŒ‰ Ctrl+C
   # é‡æ–°åŸ·è¡Œï¼Œæª¢æŸ¥æ˜¯å¦å¾ä¸Šæ¬¡ä½ç½®ç¹¼çºŒ
   ```

3. **éŒ¯èª¤è™•ç†æ¸¬è©¦**
   - æ–·ç¶²æ¸¬è©¦
   - ç„¡æ•ˆè²¼æ–‡æ¸¬è©¦
   - Cookies éæœŸæ¸¬è©¦

**é æœŸæ™‚é–“**: 2 å°æ™‚

#### 4.2 æ•ˆèƒ½å„ªåŒ–

**å„ªåŒ–é …ç›®**:

1. **æ»¾å‹•é€Ÿåº¦èª¿æ•´**
   - æ¸¬è©¦ä¸åŒå»¶é²æ˜¯å¦å½±éŸ¿æˆåŠŸç‡

2. **è¨˜æ†¶é«”ä½¿ç”¨**
   - ç›£æ§é•·æ™‚é–“é‹è¡Œçš„è¨˜æ†¶é«”

3. **æ—¥èªŒç²¾ç°¡**
   - ç§»é™¤éæ–¼è©³ç´°çš„ DEBUG æ—¥èªŒ

**é æœŸæ™‚é–“**: 1 å°æ™‚

---

### Phase 5: æ–‡æª”å®Œå–„ ğŸ“

#### 5.1 ä½¿ç”¨æ‰‹å†Š

**å»ºç«‹**: `README.md`

```markdown
# Claude Scraper å¿«é€Ÿé–‹å§‹

## å®‰è£

1. å®‰è£ä¾è³´:
   ```bash
   pip install -r requirements.txt
   playwright install chromium
   ```

2. è¨­å®šé…ç½®:
   ```bash
   cp config/config.example.json config/config.json
   # ç·¨è¼¯ config.json
   ```

3. é¦–æ¬¡ç™»å…¥:
   ```bash
   python3 src/scraper.py
   # åœ¨ç€è¦½å™¨ä¸­æ‰‹å‹•ç™»å…¥ Facebook
   ```

## ä½¿ç”¨

```bash
python3 src/scraper.py
```

## å¸¸è¦‹å•é¡Œ

è¦‹ `docs/06_troubleshooting.md`
```

#### 5.2 API åƒè€ƒ

**å»ºç«‹**: `docs/05_api_reference.md`

(åˆ—å‡ºæ‰€æœ‰é¡åˆ¥å’Œæ–¹æ³•çš„è©³ç´°èªªæ˜)

#### 5.3 æ•…éšœæ’é™¤

**å»ºç«‹**: `docs/06_troubleshooting.md`

(åˆ—å‡ºå¸¸è¦‹éŒ¯èª¤å’Œè§£æ±ºæ–¹æ³•)

#### 5.4 è®Šæ›´æ—¥èªŒ

**å»ºç«‹**: `docs/99_changelog.md`

**é æœŸæ™‚é–“**: 1.5 å°æ™‚

---

## 3. æ™‚ç¨‹ç¸½è¦½

| Phase | ä»»å‹™ | é ä¼°æ™‚é–“ | ç´¯è¨ˆæ™‚é–“ |
|-------|------|----------|----------|
| 0 | ç’°å¢ƒæº–å‚™ | 10 min | 10 min |
| 1.1 | Logger | 30 min | 40 min |
| 1.2 | State Manager | 45 min | 1h 25m |
| 1.3 | Config | 20 min | 1h 45m |
| 2.1 | Browser | 1h | 2h 45m |
| 2.2 | Extractor | 1.5h | 4h 15m |
| 3 | Main Scraper | 2h | 6h 15m |
| 4.1 | Testing | 2h | 8h 15m |
| 4.2 | Optimization | 1h | 9h 15m |
| 5 | Documentation | 1.5h | 10h 45m |

**ç¸½è¨ˆ**: ç´„ 11 å°æ™‚ (åˆ† 2-3 å¤©å®Œæˆ)

---

## 4. æª¢æŸ¥æ¸…å–®

### é–‹ç™¼å®Œæˆæª¢æŸ¥

- [ ] æ‰€æœ‰æ¨¡çµ„é€šéå–®å…ƒæ¸¬è©¦
- [ ] æ•´åˆæ¸¬è©¦æˆåŠŸ (èƒ½æŠ“å– 100+ å‰‡è²¼æ–‡)
- [ ] éŒ¯èª¤è™•ç†å®Œæ•´ (æ¨¡æ“¬å„ç¨®éŒ¯èª¤æƒ…å¢ƒ)
- [ ] æ—¥èªŒæ¸…æ™°æ˜“è®€
- [ ] ç‹€æ…‹æª”æ­£ç¢ºæ›´æ–°
- [ ] å»é‡é‚è¼¯æ­£ç¢º
- [ ] èˆ‡ Gemini ç³»çµ±æ•´åˆæˆåŠŸ

### æ–‡æª”å®Œæˆæª¢æŸ¥

- [ ] README.md å®Œæ•´
- [ ] 05_api_reference.md å®Œæ•´
- [ ] 06_troubleshooting.md åŒ…å«å¸¸è¦‹å•é¡Œ
- [ ] 99_changelog.md è¨˜éŒ„æ‰€æœ‰è®Šæ›´
- [ ] ç¨‹å¼ç¢¼è¨»è§£å……è¶³

### äº¤ä»˜æª¢æŸ¥

- [ ] requirements.txt æ­£ç¢º
- [ ] .gitignore æ­£ç¢º
- [ ] config.example.json æä¾›
- [ ] å¯åœ¨ä¹¾æ·¨ç’°å¢ƒä¸­å®‰è£ä¸¦é‹è¡Œ
- [ ] ä½¿ç”¨è€…å¯è‡ªè¡Œæ“ä½œï¼Œç„¡éœ€ AI å”åŠ©

---

## 5. é¢¨éšªæ‡‰å°

### æ½›åœ¨å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

| å•é¡Œ | è§£æ±ºæ–¹æ¡ˆ |
|------|----------|
| Facebook UI æ”¹ç‰ˆ | ä½¿ç”¨å¤šç¨®é¸æ“‡å™¨ï¼Œå®šæœŸæ¸¬è©¦ |
| Playwright ä¸ç©©å®š | é–å®šç‰ˆæœ¬ï¼Œå……åˆ†æ¸¬è©¦ |
| é–‹ç™¼æ™‚é–“è¶…å‡ºé æœŸ | å…ˆå®Œæˆ MVPï¼Œé€²éšåŠŸèƒ½å¾ŒçºŒå†åŠ  |
| ä½¿ç”¨è€…ç’°å¢ƒå•é¡Œ | æä¾›è©³ç´°çš„æ•…éšœæ’é™¤æ–‡æª” |

---

## 6. ä¸‹ä¸€æ­¥è¡Œå‹•

### ç«‹å³é–‹å§‹

1. **ç¢ºèªç’°å¢ƒæº–å‚™å®Œæˆ**
   ```bash
   python3 --version
   pip install playwright python-dotenv tqdm
   playwright install chromium
   ```

2. **å»ºç«‹ç¬¬ä¸€å€‹æ¨¡çµ„: Logger**
   - åƒè€ƒ Phase 1.1
   - å®Œæˆå¾Œæ¸¬è©¦

3. **é€æ­¥æ¨é€²**
   - æ¯å®Œæˆä¸€å€‹æ¨¡çµ„å°±æ¸¬è©¦
   - æ›´æ–° changelog

### ä½¿ç”¨è€…åƒèˆ‡é»

åœ¨ä»¥ä¸‹éšæ®µå»ºè­°ä½¿ç”¨è€…åƒèˆ‡æ¸¬è©¦:

1. **Phase 2.1 å®Œæˆå¾Œ**: æ¸¬è©¦ç€è¦½å™¨å•Ÿå‹•å’Œç™»å…¥
2. **Phase 3 å®Œæˆå¾Œ**: æ¸¬è©¦å®Œæ•´çˆ¬å–æµç¨‹ (10 å‰‡è²¼æ–‡)
3. **Phase 4.1 å®Œæˆå¾Œ**: æ¸¬è©¦å¤§è¦æ¨¡çˆ¬å– (500 å‰‡è²¼æ–‡)

---

**æ–‡ä»¶çµæŸ**

é–‹å§‹å¯¦ä½œå‰ï¼Œè«‹ç¢ºèª:
1. âœ… å·²é–±è®€æ‰€æœ‰æ–‡æª” (00-04)
2. âœ… ç†è§£æ•´é«”æ¶æ§‹
3. âœ… ç’°å¢ƒæº–å‚™å°±ç·’
4. âœ… æº–å‚™å¥½æ™‚é–“æŠ•å…¥ (2-3 å¤©)

æº–å‚™å¥½äº†å—ï¼Ÿè®“æˆ‘å€‘é–‹å§‹ Phase 0ï¼
